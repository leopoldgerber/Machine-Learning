{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc532a9",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "# Table of Contents\n",
    "\n",
    "- ### [Preprocessing-Light](#preprocessing_light)\n",
    "- ### [Preprocessing-Pandas](#preprocessing_pandas)\n",
    "- ### [Word vectorization](#word_vectorization)\n",
    "- ### [Example 1](#example_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d062bf7",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_light'></a>\n",
    "# Preprocessing-Light"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cecbfcd",
   "metadata": {},
   "source": [
    "---\n",
    "## Decoding\n",
    "converting a sequence of bytes into a sequence of characters.\n",
    "\n",
    "- **Unpacking** \\\n",
    "*.plain/.zip/.gz/...*\n",
    "- **Encoding** \\\n",
    "*ASCII/utf-8/Windows-1251/...*\n",
    "- **Format** \\\n",
    "*csv/xml/json/doc/...*\n",
    "\n",
    "---\n",
    "\n",
    "## Split into tokens\n",
    "splitting a sequence of characters into parts (tokens), possibly excluding some characters from consideration.\n",
    "Naive approach: split the string with spaces and throw out punctuation marks.\n",
    "\n",
    "**Problems:**  \n",
    "* example@example.com, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Language dependency (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "\n",
    "Alternative: n-grams\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cc9794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3ab44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      ",\n",
      "and\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sequence = 'The quick brown fox jumps, and jumps over the lazy dog'\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "for token in tokenizer.tokenize(sequence):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62e105",
   "metadata": {},
   "source": [
    "---\n",
    "## Stop words\n",
    "the most frequent words in the language that do not contain any information about the content of the text\n",
    "\n",
    "**Problem**: To be or not to be.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43b6492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(' '.join(stopwords.words('english')[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9357de5",
   "metadata": {},
   "source": [
    "---\n",
    "## Normalization\n",
    "Bringing tokens to a single form in order to get rid of superficial differences in spelling\n",
    "\n",
    "**Approaches:**\n",
    "* formulate a set of rules by which the token is transformed \\\n",
    "New-Yorker → new-yorker → newyorker → newyork\n",
    "* explicity store connections betweens tokens (WordNet - Princeton) \\\n",
    "car → auto, Window 6 → window\n",
    "машина → автомобиль, Windows 6→ window\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae967b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New-Yorker → new-yorker → newyorker → newyork\n"
     ]
    }
   ],
   "source": [
    "word = 'New-Yorker'\n",
    "\n",
    "word_1 = word.lower()\n",
    "\n",
    "import re\n",
    "word_2 = re.sub(r'\\W', '', word_1, flags = re.U)\n",
    "\n",
    "word_3 = re.sub(r'er', '', word_2, flags = re.U)\n",
    "\n",
    "print(f'{word} → {word_1} → {word_2} → {word_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeae06b",
   "metadata": {},
   "source": [
    "---\n",
    "## Stemming and Lemmatization\n",
    "**Stemming** is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\n",
    "**Lemmatization** considers the conext and converts the word to its meaningful base form, which is called *Lemma*.\n",
    "\n",
    "**Example:**\n",
    "* Steeming\n",
    "Caring → Car\n",
    "* Lemmatization\n",
    "Caring → Care\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e05f22",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfeca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Porter Stemmer]: new-york\n",
      "[Porter Stemmer]: token\n",
      "[English Stemmer]: perfect\n",
      "[English Stemmer]: differ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"New-Yorker\")}')\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"Tokenization\")}')\n",
    "\n",
    "eng_stemmer = EnglishStemmer()\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Perfection\")}')\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Difference\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af3c8c",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0278b1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pymorphy2]: new-yorker\n",
      "[pymorphy2]: tokenization\n",
      "[pymorphy2]: perfection\n",
      "[pymorphy2]: difference\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(f'[pymorphy2]: {morph.parse(\"New-Yorker\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Tokenization\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Perfection\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Difference\")[0].normal_form}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58005d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Heap's Law (Herdan's law)\n",
    "An empirical regularity in linguistics that describes the distribution of the number of unique words in a document (or set of documents) as a function of it's length.\n",
    "\n",
    "\n",
    "$ M = kT^{\\beta}$\n",
    "- $M$ - dictionary size\n",
    "- $T$ - word count\n",
    "- $30 \\leq k \\leq 100, b \\approx 0.5$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bdd03",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c27c6",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_pandas'></a>\n",
    "# Preprocessing-Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8702472",
   "metadata": {},
   "source": [
    "#### Using methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e003fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE]: Our mother washed - The Dishes\n",
      "[AFTER]: ['our', 'mother', 'washed', '-', 'the', 'dishes']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sequences_list = pd.Series(['Our mother washed - The Dishes', 'The countdown, Is over'], dtype = \"string\")\n",
    "print(f'[BEFORE]: {sequences_list[0]}')\n",
    "\n",
    "sequences_list = sequences_list.str.lower()\n",
    "sequences_list = sequences_list.str.strip()\n",
    "#sequences_list = sequences_list.str.split(' ', expand = True)\n",
    "sequences_list = sequences_list.str.split(' ')\n",
    "print(f'[AFTER]: {sequences_list[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cd15d",
   "metadata": {},
   "source": [
    "#### Using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c921e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE]: Our mother washed - The Dishes\n",
      "[AFTER]: ['our', 'mother', 'washed', 'the']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pymorphy2\n",
    "\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "sw = ['dishes']\n",
    "\n",
    "def preprocess_txt(line):\n",
    "    exclude = set(string.punctuation)\n",
    "    spls = ''.join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != '']\n",
    "    return spls\n",
    "    \n",
    "sequences_list = pd.Series(['Our mother washed - The Dishes', 'The countdown, Is over'], dtype = \"string\")\n",
    "print(f'[BEFORE]: {sequences_list[0]}')\n",
    "sequences_list = sequences_list.apply(lambda x: preprocess_txt(x))\n",
    "print(f'[AFTER]: {sequences_list[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12520a",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e0177",
   "metadata": {},
   "source": [
    "<a id = 'word_vectorization'></a>\n",
    "# Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0499a1",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9096b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1     1     0      1     0    0     1    0\n",
       "1        0      0     1   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   1     1     0      0     0    0     1    1\n",
       "3        0      0     0   1     0     1      0     1    1     0    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"I like this movie, it's funny.\", \"I hate this movie.\"\n",
    "             , \"This was awesome! I like it.\", \"Nice one. I love it.\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "count_vectorizer.fit_transform(documents)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9957f7",
   "metadata": {},
   "source": [
    "### N-gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209a37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'like')\n",
      "('like', 'this')\n",
      "('this', 'movie,')\n",
      "('movie,', \"it's\")\n",
      "(\"it's\", 'funny.')\n",
      "('funny.', 'I')\n",
      "('I', 'hate')\n",
      "('hate', 'this')\n",
      "('this', 'movie.')\n",
      "('movie.', 'This')\n",
      "('This', 'was')\n",
      "('was', 'awesome!')\n",
      "('awesome!', 'I')\n",
      "('I', 'like')\n",
      "('like', 'it.')\n",
      "('it.', 'Nice')\n",
      "('Nice', 'one.')\n",
      "('one.', 'I')\n",
      "('I', 'love')\n",
      "('love', 'it.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n",
    "\n",
    "tokenized = text.split()\n",
    "bigrams = ngrams(tokenized, 2)\n",
    "for i in bigrams:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7200501",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd2eb136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like      love     movie  \\\n",
       "0  0.000000  0.571848  0.000000  0.365003  0.450852  0.000000  0.450852   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
       "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.365003  0.000000  \n",
       "1  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "document = [\"I like this movie, it's funny.\", \"I hate this movie.\"\n",
    "            , \"This was awesome! I like it.\", \"Nice one. I love it.\"]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "feature_name = tfidf_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e0451",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5588602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1048566</th>\n",
       "      <th>1048567</th>\n",
       "      <th>1048568</th>\n",
       "      <th>1048569</th>\n",
       "      <th>1048570</th>\n",
       "      <th>1048571</th>\n",
       "      <th>1048572</th>\n",
       "      <th>1048573</th>\n",
       "      <th>1048574</th>\n",
       "      <th>1048575</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1048576 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0        1        2        3        4        5        6        7        \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   8        9        ...  1048566  1048567  1048568  1048569  1048570  \\\n",
       "0      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   1048571  1048572  1048573  1048574  1048575  \n",
       "0      0.0      0.0      0.0      0.0      0.0  \n",
       "1      0.0      0.0      0.0      0.0      0.0  \n",
       "2      0.0      0.0      0.0      0.0      0.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[4 rows x 1048576 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "document = [\"I like this movie, it's funny.\", \"I hate this movie.\"\n",
    "            , \"This was awesome! I like it.\", \"Nice one. I love it.\"]\n",
    "\n",
    "vectorizer = HashingVectorizer()\n",
    "values = vectorizer.fit_transform(document)\n",
    "\n",
    "pd.DataFrame(values.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc22b87",
   "metadata": {},
   "source": [
    "<a id = 'example_1'></a>\n",
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3be5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import datasets\n",
    "import tokenizers\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f098e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('imdb'\n",
    "                                , split = 'train'\n",
    "                                , download_mode = 'force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0725ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset\n",
    "df = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b221b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ae18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [i['label'] for i in df['train']]\n",
    "plt.hist(train_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630a639",
   "metadata": {},
   "source": [
    "### Classification using Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 25000)\n",
    "values = vectorizer.fit_transform(df['train']['text'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff36302",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 25000)\n",
    "vectorizer = vectorizer.fit(df['train']['text'])\n",
    "\n",
    "x_train = vectorizer.transform(df['train']['text'])\n",
    "y_train = np.array(df['train']['label'])\n",
    "\n",
    "x_test = vectorizer.transform(df['test']['text'])\n",
    "y_test = np.array(df['test']['label'])\n",
    "\n",
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_train, y_train)\n",
    "\n",
    "log_pred = model.predict(x_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, log_pred))\n",
    "acc = accuracy_score(y_test, log_pred)\n",
    "\n",
    "print(f'[RMSE]: {rmse:.2f}, [ACC]: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590063dd",
   "metadata": {},
   "source": [
    "#### Testing model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b56140",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_comment = 'This movie is awesome!'\n",
    "vec = vectorizer.transform([positive_comment])\n",
    "positive_pred = model.predict(vec)\n",
    "\n",
    "negative_comment = 'This movie is awful!'\n",
    "vec = vectorizer.transform([negative_comment])\n",
    "negative_pred = model.predict(vec)\n",
    "\n",
    "print(f'[PositiveComment]: {positive_pred[0]}, \\\n",
    "[NegativeComment]: {negative_pred[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe83cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "iso = Isomap(n_neighbors = 2, eigen_solver = 'auto')\n",
    "new = iso.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee67ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204c25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a9b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5b09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ccdb20f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ACC]: 0.75, [RMSE]: 0.25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>best</th>\n",
       "      <th>dont</th>\n",
       "      <th>ever</th>\n",
       "      <th>hate</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>life</th>\n",
       "      <th>liked</th>\n",
       "      <th>...</th>\n",
       "      <th>my</th>\n",
       "      <th>new</th>\n",
       "      <th>really</th>\n",
       "      <th>refreshing</th>\n",
       "      <th>saw</th>\n",
       "      <th>something</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>times</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.322673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388722</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and      best      dont      ever      hate        in        is        it  \\\n",
       "0  0.0  0.000000  0.000000  0.000000  0.634533  0.000000  0.000000  0.000000   \n",
       "1  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.0  0.397303  0.000000  0.397303  0.000000  0.000000  0.478629  0.000000   \n",
       "3  0.0  0.000000  0.000000  0.000000  0.000000  0.456692  0.000000  0.000000   \n",
       "4  0.5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.0  0.000000  0.609819  0.000000  0.000000  0.000000  0.000000  0.506202   \n",
       "6  0.0  0.322673  0.000000  0.322673  0.000000  0.000000  0.000000  0.322673   \n",
       "\n",
       "       life     liked  ...        my  new    really  refreshing       saw  \\\n",
       "0  0.000000  0.000000  ...  0.000000  0.0  0.526716         0.0  0.000000   \n",
       "1  0.000000  0.000000  ...  0.000000  0.0  0.526716         0.0  0.000000   \n",
       "2  0.000000  0.000000  ...  0.000000  0.0  0.000000         0.0  0.397303   \n",
       "3  0.456692  0.000000  ...  0.456692  0.0  0.000000         0.0  0.000000   \n",
       "4  0.000000  0.000000  ...  0.000000  0.5  0.000000         0.5  0.000000   \n",
       "5  0.000000  0.609819  ...  0.000000  0.0  0.000000         0.0  0.000000   \n",
       "6  0.000000  0.000000  ...  0.000000  0.0  0.000000         0.0  0.645346   \n",
       "\n",
       "   something       the      this     times     worst  \n",
       "0        0.0  0.000000  0.450220  0.000000  0.000000  \n",
       "1        0.0  0.000000  0.450220  0.000000  0.000000  \n",
       "2        0.0  0.339602  0.339602  0.000000  0.000000  \n",
       "3        0.0  0.324036  0.000000  0.000000  0.456692  \n",
       "4        0.5  0.000000  0.000000  0.000000  0.000000  \n",
       "5        0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "6        0.0  0.275810  0.000000  0.388722  0.000000  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_new = ['I really hate this movie', 'I really love this movie.'\n",
    "            , 'This is the best movie I ever saw!', 'The worst movie in my life!'\n",
    "            , 'something new and refreshing', 'i dont liked it'\n",
    "            , 'i saw it 4 times the best ever movie i saw!']\n",
    "label_new = [0, 1, 1, 0, 1, 0, 1]\n",
    "\n",
    "text_new_test = ['I love this movie', 'Best movie I ever saw!'\n",
    "                  , 'I really hated this movie.', 'Worst movie in my life!']\n",
    "label_new_test = [1, 1, 0, 0]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf = tfidf.fit(text_new)\n",
    "\n",
    "x_train = tfidf.transform(text_new)\n",
    "y_train = np.array(label_new)\n",
    "\n",
    "x_test = tfidf.transform(text_new_test)\n",
    "y_test = np.array(label_new_test)\n",
    "\n",
    "new_df = pd.DataFrame(x_train.toarray(), columns = tfidf.get_feature_names_out())\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(f'[ACC]: {accuracy_score(y_pred, y_test):.2f}, [RMSE]: {mean_squared_error(y_pred, y_test):.2f}')\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "5dbf0929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAle0lEQVR4nO3de3TV5Z3v8c/ObSeBZHMJSXYkhOBA5KJMDFVChapMI6FSaz0drDbGjqWDU+vBlGNFp/WyVle0w1iWg8DQoi6L55TTBjm6oNScGULoIVTBUG25iGMkERJDIuQq2SE854+UbWN2bpBf9n4279dav9Xu5/f88vs+fSj58Pwu22WMMQIAALBERLALAAAAGArCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKlHBLmC4nT9/XidPnlRCQoJcLlewywEAAINgjFFLS4vS0tIUEdH/2krYhZeTJ08qPT092GUAAICLUFNTo4kTJ/bbJ+zCS0JCgqTuwScmJga5GgAAMBjNzc1KT0/3/x7vT9iFlwuXihITEwkvAABYZjC3fHDDLgAAsArhBQAAWIXwAgAArBJ297wAAGCrrq4udXZ2BrsMx0RGRioqKuqSX2VCeAEAIAS0trbqo48+kjEm2KU4Kj4+Xl6vVzExMRf9MwgvAAAEWVdXlz766CPFx8drwoQJYfmSVWOMfD6fTp06paqqKk2dOnXAl9H1hfACAECQdXZ2yhijCRMmKC4uLtjlOCYuLk7R0dE6fvy4fD6fYmNjL+rnOHrDbnl5uZYsWaK0tDS5XC5t27at3/5lZWVyuVy9tiNHjjhZJgAAISEcV1w+72JXW/6aoysvbW1tmj17tr797W/rjjvuGPRxR48e7fGCuQkTJjhRHgAAsJCj4SU/P1/5+flDPi45OVljxowZ/oKAy8iZdp8aW30686lPCbHRShodo3Gj3MEuCwAuWUje85Kdna2zZ89qxowZ+ud//mfddNNNffbt6OhQR0eH/3Nzc/NIlAiEtNqmT/VIybva/d4pf1v2pDH6t29ma+LY+CBWBgCXLqReUuf1erVx40aVlJRo69atysrK0sKFC1VeXt7nMcXFxfJ4PP6Nb5TG5a7lbKeefO1Qj+AiSZXVZ/S9//m2Gls7+jgSAC7OunXrlJmZqdjYWOXk5GjPnj2Oni+kVl6ysrKUlZXl/5ybm6uamhqtXr1aCxYsCHjMqlWrVFRU5P984VspgctVQ6tPvztUF3DfH2ua1NDaofGjuXwEhKOu80ZvVn2i+pazSk6I1XWZ4xQZ4exNwFu2bNGKFSu0bt06ffGLX9S///u/Kz8/X4cOHdKkSZMcOWdIhZdA5s6dq82bN/e53+12y+3mL2LggtaOTvX3jqtP2nwjVwyAEbPzT7V68vVDqm0662/zemL1+JIZWjTL69h5n332Wd133336zne+I0las2aNfve732n9+vUqLi525JwhddkokMrKSnm9zv2PDoSbhNho9fcPLVZdgPCz80+1un/z2z2CiyTVNZ3V/Zvf1s4/1TpyXp/PpwMHDigvL69He15envbu3evIOSWHV15aW1v1/vvv+z9XVVXp4MGDGjdunCZNmqRVq1bpxIkTevnllyV1p7XJkydr5syZ8vl82rx5s0pKSlRSUuJkmUBYSRrt1pJr0vR//niy174vTB6rpNEX/0puAKGn67zRk68fUqAFVyPJJenJ1w/pyzNSh/0SUkNDg7q6upSSktKjPSUlRXV1gS9fDwdHw8v+/ft7PCl04d6UwsJCvfTSS6qtrVV1dbV/v8/n08qVK3XixAnFxcVp5syZ2r59uxYvXuxkmUBYGe2O0qNfma4uY7T93Vr/JaT5U5P0zB3X8Lg0EGberPqk14rLXzOSapvO6s2qT5R75XhHavj8y/WMMY6+cM/R8HLjjTf2+wVTL730Uo/PDz/8sB5++GEnSwIuCymJsSr++tX6Qd40NX96TqNjozR+VIzGxLPqAoSb+pa+g8vF9BuKpKQkRUZG9lplqa+v77UaM5xC/p4XABcnITZamUmjNTt9jK6cMJrgAoSp5ITBfT/QYPsNRUxMjHJyclRaWtqjvbS0VPPmzRv2810Q8k8bAQCAvl2XOU5eT6zqms4GvO/FJSnV0/3YtBOKiopUUFCgOXPmKDc3Vxs3blR1dbWWL1/uyPkkwgsAAFaLjHDp8SUzdP/mt+WSegSYC3edPL5khmPve1m6dKkaGxv11FNPqba2VrNmzdKOHTuUkZHhyPkkLhsBAGC9RbO8Wv+ta5Xq6XlpKNUTq/XfutbR97xI0j/90z/pww8/VEdHhw4cONDni2WHCysvAACEgUWzvPryjNQRf8NuMBBeAAAIE5ERLscehw4lXDYCAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAC5aeXm5lixZorS0NLlcLm3bts3xcxJeAAAIF+e7pKo90ru/6f7P812On7KtrU2zZ8/W2rVrHT/XBbxhFwCAcHDoNWnnD6Xmk5+1JaZJi56RZnzVsdPm5+crPz/fsZ8fCCsvAADY7tBr0v++p2dwkaTm2u72Q68Fpy6HEF4AALDZ+a7uFReZADv/0rbzkRG5hDRSCC8AANjs+N7eKy49GKn5RHe/MEF4AQDAZq0fD28/CxBeAACw2eiU4e1nAZ42AgDAZhnzup8qaq5V4PteXN37M+Y5cvrW1la9//77/s9VVVU6ePCgxo0bp0mTJjlyTlZeAACwWURk9+PQkiTX53b+5fOip7v7OWD//v3Kzs5Wdna2JKmoqEjZ2dn68Y9/7Mj5JFZeAACw34yvSn//ch/veXna0fe83HjjjTIm0IqPcwgvAACEgxlfla76SvdTRa0fd9/jkjHPsRWXYCK8AAAQLiIipcz5wa7CcdzzAgAArEJ4AQAAViG8AAAAqxBeAAAIESP91E4wDMcYCS8AAARZZGT3E0E+ny/IlTivvb1dkhQdHX3RP4OnjQAACLKoqCjFx8fr1KlTio6OVkRE+K0tGGPU3t6u+vp6jRkzxh/YLgbhBQCAIHO5XPJ6vaqqqtLx48eDXY6jxowZo9TU1Ev6GYQXAABCQExMjKZOnRrWl46io6MvacXlAsILAAAhIiIiQrGxscEuI+SF30U1AAAQ1ggvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYhvAAAAKsQXgAAgFUILwAAwCqEFwAAYBXCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrOBpeysvLtWTJEqWlpcnlcmnbtm0DHrN7927l5OQoNjZWU6ZM0YYNG5wsEQAAWMbR8NLW1qbZs2dr7dq1g+pfVVWlxYsXa/78+aqsrNSjjz6qBx98UCUlJU6WCQAALBLl5A/Pz89Xfn7+oPtv2LBBkyZN0po1ayRJ06dP1/79+7V69WrdcccdDlUJAABsElL3vFRUVCgvL69H2y233KL9+/ers7Mz4DEdHR1qbm7usQEAgPAVUuGlrq5OKSkpPdpSUlJ07tw5NTQ0BDymuLhYHo/Hv6Wnp49EqQAAIEhCKrxIksvl6vHZGBOw/YJVq1apqanJv9XU1DheIwAACB5H73kZqtTUVNXV1fVoq6+vV1RUlMaPHx/wGLfbLbfbPRLlAQCAEBBSKy+5ubkqLS3t0fbGG29ozpw5io6ODlJVAAAglDgaXlpbW3Xw4EEdPHhQUvej0AcPHlR1dbWk7ks+99xzj7//8uXLdfz4cRUVFenw4cN64YUXtGnTJq1cudLJMgEAgEUcvWy0f/9+3XTTTf7PRUVFkqTCwkK99NJLqq2t9QcZScrMzNSOHTv00EMP6fnnn1daWpqee+45HpMGAAB+LnPhjtgw0dzcLI/Ho6amJiUmJga7HAAAMAhD+f0dUve8AAAADITwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYhvAAAAKsQXgAAgFUILwAAwCqEFwAAYBXCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYhvAAAAKsQXgAAgFUILwAAwCqEFwAAYBXCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYZkfCybt06ZWZmKjY2Vjk5OdqzZ0+ffcvKyuRyuXptR44cGYlSAQBAiHM8vGzZskUrVqzQY489psrKSs2fP1/5+fmqrq7u97ijR4+qtrbWv02dOtXpUgEAgAUcDy/PPvus7rvvPn3nO9/R9OnTtWbNGqWnp2v9+vX9HpecnKzU1FT/FhkZ6XSpAADAAo6GF5/PpwMHDigvL69He15envbu3dvvsdnZ2fJ6vVq4cKF27drVZ7+Ojg41Nzf32AAAQPhyNLw0NDSoq6tLKSkpPdpTUlJUV1cX8Biv16uNGzeqpKREW7duVVZWlhYuXKjy8vKA/YuLi+XxePxbenr6sI8DAACEjqiROInL5erx2RjTq+2CrKwsZWVl+T/n5uaqpqZGq1ev1oIFC3r1X7VqlYqKivyfm5ubCTAAAIQxR1dekpKSFBkZ2WuVpb6+vtdqTH/mzp2rY8eOBdzndruVmJjYYwMAAOHL0fASExOjnJwclZaW9mgvLS3VvHnzBv1zKisr5fV6h7s8AABgIccvGxUVFamgoEBz5sxRbm6uNm7cqOrqai1fvlxS92WfEydO6OWXX5YkrVmzRpMnT9bMmTPl8/m0efNmlZSUqKSkxOlSAQCABRwPL0uXLlVjY6Oeeuop1dbWatasWdqxY4cyMjIkSbW1tT3e+eLz+bRy5UqdOHFCcXFxmjlzprZv367Fixc7XSoAALCAyxhjgl3EcGpubpbH41FTUxP3vwAAYImh/P7mu40AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYhvAAAAKsQXgAAgFUILwAAwCqOf7cRQtjZZqntlHTqqBQdL42fIo1OlaJigl0ZAAB9IrxcrtoapP+3RqpYK134eqvoeOm/vSBNuVGKjgtmdQAA9InLRperD8qkvf/2WXCRpM52acvdUlNN0MoCAGAghJfLUespqfyngfed75L+uGVk6wEAYAgIL5ej851S04m+9zcclc6fH7l6AAAYAsLL5Sg6Tkq9uu/9mQukCP5oAABCE7+hLkdxY6W/ezLwvtgx0tRbRrQcAACGgvByuUqZKS19RUpI/azNO1v69g5pzKTg1QUAwAB4VPpy5R4tXfUV6YprpU/PSJFRUtw4aVRSsCsDAKBfhJfLmcslJaZ1bwAAWILLRgAAwCqEFwAAYBXCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALBKVLALAIDLXsvH0vlOKSJaSkgJdjVAyCO8AECwtDVK//Uf0q6fSKc/lMZNkW56TLryZil+XLCrA0IWl40AIBh87dJbv5C2LusOLpL0yQdSyX3S2y9LnWeDWh4QyggvABAMbaekPasD7ysrllo/Htl6AIsQXgAgGNpOSV2+wPvOnZXaG0e2HsAihBcACIbImEvbD1zGCC8AEAyjJkieiYH3jZ0sjUoa0XIAmxBeACAYEr3S32+WYkb3bHcnSn//SykhNTh1ARbgUWkACBbvNdL9e6Wq3dLJg9IV10qT50ue9GBXBoQ0wgsABEtEpDQ2Qxp7j3TtPcGuBrAGl40AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYhvAAAAKsQXgAAgFUILwAAwCqEFwAAYBXCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKiMSXtatW6fMzEzFxsYqJydHe/bs6bf/7t27lZOTo9jYWE2ZMkUbNmwYiTIBAIAFHA8vW7Zs0YoVK/TYY4+psrJS8+fPV35+vqqrqwP2r6qq0uLFizV//nxVVlbq0Ucf1YMPPqiSkhKnSwUAABZwGWOMkye4/vrrde2112r9+vX+tunTp+trX/uaiouLe/X/4Q9/qNdee02HDx/2ty1fvlx//OMfVVFRMeD5mpub5fF41NTUpMTExOEZBAAAcNRQfn87uvLi8/l04MAB5eXl9WjPy8vT3r17Ax5TUVHRq/8tt9yi/fv3q7Ozs1f/jo4ONTc399gAAED4cjS8NDQ0qKurSykpKT3aU1JSVFdXF/CYurq6gP3PnTunhoaGXv2Li4vl8Xj8W3p6+vANAAAAhJwRuWHX5XL1+GyM6dU2UP9A7ZK0atUqNTU1+beampphqBgAAISqKCd/eFJSkiIjI3utstTX1/daXbkgNTU1YP+oqCiNHz++V3+32y232z18RQMAgJDm6MpLTEyMcnJyVFpa2qO9tLRU8+bNC3hMbm5ur/5vvPGG5syZo+joaMdqBQAAdnD8slFRUZF+8Ytf6IUXXtDhw4f10EMPqbq6WsuXL5fUfdnnnnvu8fdfvny5jh8/rqKiIh0+fFgvvPCCNm3apJUrVzpdKgAAsICjl40kaenSpWpsbNRTTz2l2tpazZo1Szt27FBGRoYkqba2tsc7XzIzM7Vjxw499NBDev7555WWlqbnnntOd9xxh9OlAgAACzj+npeRxnteAACwT8i85wUAAGC4EV4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYhvAAAAKsQXgAAgFUILwAAwCqEFwAAYBXCCwAAsEpUsAsAAAAW8LVJbaekT6qkyGhpzCRpdKoUFTPipRBeAABA/z49Lb39S+k/npTOn+tui46Xbt8g/c3fSTGjRrQcLhsBAID+nTwolf7os+AiSZ3t0q8LpTPHR7wcwgsAAOjbp2ek3T8NvM8Y6c1NUte5wPsdQngBAAB9O3e2/9WVxmNSV8fI1SPCCwAA6E/MKCn1mr73T7xOiooduXpEeAEAAP1xJ0g3/lByuXrvi46T/vYuKSJyREsivAAAgP4lTZXu/JWUkPpZ2/grpcLXux+ZHmE8Kg0AAPoXM1qadou0bJfU/kn3SkvcOCkhJSjlEF4AAMDAXC4pMa17CzIuGwEAAKsQXgAAgFUILwAAwCqEFwAAYBXCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFiF8AIAAKxCeAEAAFYhvAAAAKsQXgAAgFUILwAAwCqEFwAAYBXCCwAAsArhBQAAWIXwAgAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVR8PL6dOnVVBQII/HI4/Ho4KCAp05c6bfY+699165XK4e29y5c50sEwAAWCTKyR9+11136aOPPtLOnTslSd/97ndVUFCg119/vd/jFi1apBdffNH/OSYmxskyAQCARRwLL4cPH9bOnTu1b98+XX/99ZKkn//858rNzdXRo0eVlZXV57Fut1upqalOlQYAACzm2GWjiooKeTwef3CRpLlz58rj8Wjv3r39HltWVqbk5GRNmzZNy5YtU319vVNlAgAAyzi28lJXV6fk5ORe7cnJyaqrq+vzuPz8fH3jG99QRkaGqqqq9KMf/Ug333yzDhw4ILfb3at/R0eHOjo6/J+bm5uHZwAAACAkDXnl5Yknnuh1Q+3nt/3790uSXC5Xr+ONMQHbL1i6dKm+8pWvaNasWVqyZIl++9vf6r333tP27dsD9i8uLvbfEOzxeJSenj7UIQEAAIsMeeXlgQce0J133tlvn8mTJ+udd97Rxx9/3GvfqVOnlJKSMujzeb1eZWRk6NixYwH3r1q1SkVFRf7Pzc3NBBgAAMLYkMNLUlKSkpKSBuyXm5urpqYmvfnmm7ruuuskSX/4wx/U1NSkefPmDfp8jY2NqqmpkdfrDbjf7XYHvJwEAADCk2M37E6fPl2LFi3SsmXLtG/fPu3bt0/Lli3Trbfe2uNJo6uuukqvvvqqJKm1tVUrV65URUWFPvzwQ5WVlWnJkiVKSkrS7bff7lSpAADAIo6+pO6VV17R1Vdfrby8POXl5emaa67RL3/5yx59jh49qqamJklSZGSk3n33Xd12222aNm2aCgsLNW3aNFVUVCghIcHJUgEAgCVcxhgT7CKGU3Nzszwej5qampSYmBjscgAAwCAM5fc3320EAACsQngBAABWIbwAAACrEF4AAIBVHP1WaeCy1fmp1HRCOrRNajgmXXmTlPFFaQwvUASAS0V4AYbbOZ/0QZm05W7pfFd32zu/kkZNkL69U0r6m6CWBwC247IRMNxaaqVf3/tZcLmg7ZT0+n+XPj0dlLJgh67zRrVNn6q6sV0fN51VmL3NAhgWrLwAw63+sHTubOB9x38vtX8ixY0d2ZpghVMtHdr69kfasPu/dLq9U15PrH7w5Wm6eXqKxo2KCXZ5QMhg5QUYbr7W/vd3dY5MHbBKy9lO/az0PRX/9ohOt3f/GaltOquVv3lHvzlQI9+5rgF+AnD5ILwAwy316r73eSZKsZ6RqwXWaGj16X+9VR1w35r/e0z1LR0jXBEQuggvwHAbnSz97bcC78v/Fykx8Dek4/J28syn6uv2lnZfl860s2IHXEB4AYZb3Fjp756Qbv2ZNGaSFBktTfyC9O3fSpkLgl0dQtRod/+3ILqj+esauIAbdgEnjJ4gzfkHKWtx91NH0XFS/LhgV4UQlpLoVnKCO+DloVlXJGo8N+wCfkR5wEkJqZLnCoILBpSSGKtNhXOU8LkVmOQEt567M1vjRrmDVBkQelh5AYAQ4HK5NDPNo9+umK/K6jN6v75FV08coxneRKWNiQt2eUBIIbwMQue586pvOauGVp+MMZqQ4NaERLdiIiODXRqAMBIR4dLEsfGaODY+2KUAIY3wMoB23zmVv3dK/+PX76il45wkaVRMpIq/frVunp4y4E12AABgeHHPywCON7br/lfe9gcXSWrzdenBXx1U1akBXkYGAACGHeGlHx2dXfr5ng/6fPfCurL/UvtfhRoAAOA8wks/PvV16djHfa+uvF/fqk87eWU3AAAjifDSj3h3lGZ4E/rcf1VqguLd3LQLAMBIIrz0IyYqQv9wwxRFRrh67XO5pPtv/BvFRXPDLgAAI4nwMoBJ4+O1qXCOkkZ/9nbLsfHR2vitHE1O4nFGAABGGssGA4iLjtSCqRP0+gM3qLHNJ0kaNypGKYmxAVdkAACAswgvgxAR4ZJ3TJy8vOUSAICg47IRAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBVCC8AAMAqhBcAAGAVwgsAALAK4QUAAFgl7L4ewBgjSWpubg5yJQAAYLAu/N6+8Hu8P2EXXlpaWiRJ6enpQa4EAAAMVUtLizweT799XGYwEcci58+f18mTJ5WQkCCXy65vfW5ublZ6erpqamqUmJgY7HIcwzjDy+UwzsthjBLjDDe2jdMYo5aWFqWlpSkiov+7WsJu5SUiIkITJ04MdhmXJDEx0Yo/aJeKcYaXy2Gcl8MYJcYZbmwa50ArLhdwwy4AALAK4QUAAFiF8BJC3G63Hn/8cbnd7mCX4ijGGV4uh3FeDmOUGGe4Cedxht0NuwAAILyx8gIAAKxCeAEAAFYhvAAAAKsQXgAAgFUIL0H0k5/8RPPmzVN8fLzGjBkzqGPuvfdeuVyuHtvcuXOdLfQSXcw4jTF64oknlJaWpri4ON14443685//7Gyhl+j06dMqKCiQx+ORx+NRQUGBzpw50+8xNsznunXrlJmZqdjYWOXk5GjPnj399t+9e7dycnIUGxurKVOmaMOGDSNU6aUZyjjLysp6zZvL5dKRI0dGsOKhKy8v15IlS5SWliaXy6Vt27YNeIyN8znUcdo4n8XFxfrCF76ghIQEJScn62tf+5qOHj064HE2zmcghJcg8vl8+sY3vqH7779/SMctWrRItbW1/m3Hjh0OVTg8LmacP/3pT/Xss89q7dq1euutt5Samqovf/nL/u+uCkV33XWXDh48qJ07d2rnzp06ePCgCgoKBjwulOdzy5YtWrFihR577DFVVlZq/vz5ys/PV3V1dcD+VVVVWrx4sebPn6/Kyko9+uijevDBB1VSUjLClQ/NUMd5wdGjR3vM3dSpU0eo4ovT1tam2bNna+3atYPqb+t8DnWcF9g0n7t379b3vvc97du3T6WlpTp37pzy8vLU1tbW5zG2zmdABkH34osvGo/HM6i+hYWF5rbbbnO0HqcMdpznz583qamp5umnn/a3nT171ng8HrNhwwYHK7x4hw4dMpLMvn37/G0VFRVGkjly5Eifx4X6fF533XVm+fLlPdquuuoq88gjjwTs//DDD5urrrqqR9s//uM/mrlz5zpW43AY6jh37dplJJnTp0+PQHXOkGReffXVfvvYOp9/bTDjDIf5rK+vN5LM7t27++wTDvN5ASsvFiorK1NycrKmTZumZcuWqb6+PtglDauqqirV1dUpLy/P3+Z2u/WlL31Je/fuDWJlfauoqJDH49H111/vb5s7d648Hs+ANYfqfPp8Ph04cKDHPEhSXl5en2OqqKjo1f+WW27R/v371dnZ6Vitl+JixnlBdna2vF6vFi5cqF27djlZZlDYOJ+Xwub5bGpqkiSNGzeuzz7hNJ+EF8vk5+frlVde0X/+53/qX//1X/XWW2/p5ptvVkdHR7BLGzZ1dXWSpJSUlB7tKSkp/n2hpq6uTsnJyb3ak5OT+605lOezoaFBXV1dQ5qHurq6gP3PnTunhoYGx2q9FBczTq/Xq40bN6qkpERbt25VVlaWFi5cqPLy8pEoecTYOJ8Xw/b5NMaoqKhIN9xwg2bNmtVnv3Caz7D7Vulge+KJJ/Tkk0/22+ett97SnDlzLurnL1261P/fZ82apTlz5igjI0Pbt2/X17/+9Yv6mRfD6XFKksvl6vHZGNOrzWmDHafUu15p4JpDZT77M9R5CNQ/UHuoGco4s7KylJWV5f+cm5urmpoarV69WgsWLHC0zpFm63wOhe3z+cADD+idd97R73//+wH7hst8El6G2QMPPKA777yz3z6TJ08etvN5vV5lZGTo2LFjw/YzB8PJcaampkrq/leC1+v1t9fX1/f6V4PTBjvOd955Rx9//HGvfadOnRpSzcGaz0CSkpIUGRnZa/Whv3lITU0N2D8qKkrjx493rNZLcTHjDGTu3LnavHnzcJcXVDbO53CxZT6///3v67XXXlN5ebkmTpzYb99wmk/CyzBLSkpSUlLSiJ2vsbFRNTU1PX7JjwQnx5mZmanU1FSVlpYqOztbUvd9Cbt379YzzzzjyDn7Mthx5ubmqqmpSW+++aauu+46SdIf/vAHNTU1ad68eYM+X7DmM5CYmBjl5OSotLRUt99+u7+9tLRUt912W8BjcnNz9frrr/doe+ONNzRnzhxFR0c7Wu/FuphxBlJZWRkS8zacbJzP4RLq82mM0fe//329+uqrKisrU2Zm5oDHhNV8Bu1WYZjjx4+byspK8+STT5rRo0ebyspKU1lZaVpaWvx9srKyzNatW40xxrS0tJgf/OAHZu/evaaqqsrs2rXL5ObmmiuuuMI0NzcHaxgDGuo4jTHm6aefNh6Px2zdutW8++675pvf/Kbxer0hPc5FixaZa665xlRUVJiKigpz9dVXm1tvvbVHH9vm81e/+pWJjo42mzZtMocOHTIrVqwwo0aNMh9++KExxphHHnnEFBQU+Pt/8MEHJj4+3jz00EPm0KFDZtOmTSY6Otr85je/CdYQBmWo4/zZz35mXn31VfPee++ZP/3pT+aRRx4xkkxJSUmwhjAoLS0t/v//STLPPvusqaysNMePHzfGhM98DnWcNs7n/fffbzwejykrKzO1tbX+rb293d8nXOYzEMJLEBUWFhpJvbZdu3b5+0gyL774ojHGmPb2dpOXl2cmTJhgoqOjzaRJk0xhYaGprq4OzgAGaajjNKb7cenHH3/cpKamGrfbbRYsWGDefffdkS9+CBobG83dd99tEhISTEJCgrn77rt7PXpp43w+//zzJiMjw8TExJhrr722x6OYhYWF5ktf+lKP/mVlZSY7O9vExMSYyZMnm/Xr149wxRdnKON85plnzJVXXmliY2PN2LFjzQ033GC2b98ehKqH5sIjwZ/fCgsLjTHhM59DHaeN8xlofJ//ezRc5jMQlzF/uVsHAADAAjwqDQAArEJ4AQAAViG8AAAAqxBeAACAVQgvAADAKoQXAABgFcILAACwCuEFAABYhfACAACsQngBAABWIbwAAACrEF4AAIBV/j+cfMfLStL6tAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "iso = Isomap(n_neighbors = 2, eigen_solver = 'auto')\n",
    "train_reduce = iso.fit_transform(x_train.toarray())\n",
    "\n",
    "sns.scatterplot(x = train_reduce[:, 0], y = train_reduce[:, 1], hue = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d84c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "# Insert your TF-IDF vectorizing here\n",
    "\n",
    "##\n",
    "# Do the dimension reduction\n",
    "##\n",
    "k = 10 # number of nearest neighbors to consider\n",
    "d = 2 # dimensionality\n",
    "pos = manifold.Isomap(k, d, eigen_solver='auto').fit_transform(.toarray())\n",
    "\n",
    "##\n",
    "# Get meaningful \"cluster\" labels\n",
    "##\n",
    "#Semantic labeling of cluster. Apply a label if the clusters max TF-IDF is in the 99% quantile of the whole corpus of TF-IDF scores\n",
    "labels = vectorizer.get_feature_names() #text labels of features\n",
    "clusterLabels = []\n",
    "t99 = scipy.stats.mstats.mquantiles(X.data, [ 0.99])[0]\n",
    "clusterLabels = []\n",
    "for i in range(0,vectorized.shape[0]):\n",
    "    row = vectorized.getrow(i)\n",
    "    if row.max() >= t99:\n",
    "        arrayIndex = numpy.where(row.data == row.max())[0][0]\n",
    "        clusterLabels.append(labels[row.indices[arrayIndex]])\n",
    "    else:\n",
    "        clusterLabels.append('')\n",
    "##\n",
    "# Plot the dimension reduced data\n",
    "##\n",
    "pyplot.xlabel('reduced dimension-1')\n",
    "pyplot.ylabel('reduced dimension-2')\n",
    "for i in range(1, len(pos)):\n",
    "    pyplot.scatter(pos[i][0], pos[i][1], c='cyan')\n",
    "    pyplot.annotate(clusterLabels[i], pos[i], xytext=None, xycoords='data', textcoords='data', arrowprops=None)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf0892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c7417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6989b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca539e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "771b4c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>hate</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>life</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>my</th>\n",
       "      <th>really</th>\n",
       "      <th>saw</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.659191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420753</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.659191</td>\n",
       "      <td>0.343993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420753</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.434318</td>\n",
       "      <td>0.434318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434318</td>\n",
       "      <td>0.342421</td>\n",
       "      <td>0.277220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235891</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       best      ever      hate        in        is      life      love  \\\n",
       "0  0.000000  0.000000  0.659191  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.659191   \n",
       "2  0.434318  0.434318  0.000000  0.000000  0.434318  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.452035  0.000000  0.452035  0.000000   \n",
       "\n",
       "      movie        my    really       saw       the      this     worst  \n",
       "0  0.343993  0.000000  0.519714  0.000000  0.000000  0.420753  0.000000  \n",
       "1  0.343993  0.000000  0.519714  0.000000  0.000000  0.420753  0.000000  \n",
       "2  0.226645  0.000000  0.000000  0.434318  0.342421  0.277220  0.000000  \n",
       "3  0.235891  0.452035  0.000000  0.000000  0.356389  0.000000  0.452035  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916ac44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c195c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcb89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c8ba4cf",
   "metadata": {},
   "source": [
    "<a id = 'kmeans_inertia'></a>\n",
    "<left>\n",
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border: 0px;\n",
    "           border-bottom: 2px solid #AAA;\n",
    "           font-size:80%;\n",
    "           letter-spacing:0.5px\">\n",
    "<h2 style=\"padding: 10px;\n",
    "           color:#212121;\">Inertia\n",
    "</h2>\n",
    "</div>    \n",
    "</left>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
