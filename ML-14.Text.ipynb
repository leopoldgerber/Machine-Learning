{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc532a9",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "# Table of Contents\n",
    "\n",
    "- ### [Preprocessing-Light](#preprocessing_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d062bf7",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_light'></a>\n",
    "# Preprocessing-Light"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975fd44",
   "metadata": {},
   "source": [
    "---\n",
    "## Decoding\n",
    "converting a sequence of bytes into a sequence of characters.\n",
    "\n",
    "- **Unpacking** \\\n",
    "*.plain/.zip/.gz/...*\n",
    "- **Encoding** \\\n",
    "*ASCII/utf-8/Windows-1251/...*\n",
    "- **Format** \\\n",
    "*csv/xml/json/doc/...*\n",
    "\n",
    "---\n",
    "\n",
    "## Split into tokens\n",
    "splitting a sequence of characters into parts (tokens), possibly excluding some characters from consideration.\n",
    "Naive approach: split the string with spaces and throw out punctuation marks.\n",
    "\n",
    "**Problems:**  \n",
    "* example@example.com, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Language dependency (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "\n",
    "Alternative: n-grams\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9dafe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a316e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      ",\n",
      "and\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sequence = 'The quick brown fox jumps, and jumps over the lazy dog'\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "for token in tokenizer.tokenize(sequence):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee7ea5",
   "metadata": {},
   "source": [
    "---\n",
    "## Stop words\n",
    "the most frequent words in the language that do not contain any information about the content of the text\n",
    "\n",
    "**Problem**: To be or not to be.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0847a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(' '.join(stopwords.words('english')[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72161d71",
   "metadata": {},
   "source": [
    "---\n",
    "## Normalization\n",
    "Bringing tokens to a single form in order to get rid of superficial differences in spelling\n",
    "\n",
    "**Approaches:**\n",
    "* formulate a set of rules by which the token is transformed \\\n",
    "New-Yorker → new-yorker → newyorker → newyork\n",
    "* explicity store connections betweens tokens (WordNet - Princeton) \\\n",
    "car → auto, Window 6 → window\n",
    "машина → автомобиль, Windows 6→ window\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "622180c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New-Yorker → new-yorker → newyorker → newyork\n"
     ]
    }
   ],
   "source": [
    "word = 'New-Yorker'\n",
    "\n",
    "word_1 = word.lower()\n",
    "\n",
    "import re\n",
    "word_2 = re.sub(r'\\W', '', word_1, flags = re.U)\n",
    "\n",
    "word_3 = re.sub(r'er', '', word_2, flags = re.U)\n",
    "\n",
    "print(f'{word} → {word_1} → {word_2} → {word_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f2fae",
   "metadata": {},
   "source": [
    "---\n",
    "## Stemming and Lemmatization\n",
    "**Stemming** is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\n",
    "**Lemmatization** considers the conext and converts the word to its meaningful base form, which is called *Lemma*.\n",
    "\n",
    "**Example:**\n",
    "* Steeming\n",
    "Caring → Car\n",
    "* Lemmatization\n",
    "Caring → Care\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf2d58",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96674bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178bd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf748e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e075eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17921832",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa750c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543fcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066b948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116084f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5d992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2830a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea281c60",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee5e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6e436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cbfd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28635679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66437053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98633e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15bc94f7",
   "metadata": {},
   "source": [
    "<a id = 'kmeans_inertia'></a>\n",
    "<left>\n",
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border: 0px;\n",
    "           border-bottom: 2px solid #AAA;\n",
    "           font-size:80%;\n",
    "           letter-spacing:0.5px\">\n",
    "<h2 style=\"padding: 10px;\n",
    "           color:#212121;\">Inertia\n",
    "</h2>\n",
    "</div>    \n",
    "</left>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
