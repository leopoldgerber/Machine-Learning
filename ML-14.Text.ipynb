{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc532a9",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "# Table of Contents\n",
    "\n",
    "- ### [Preprocessing-Light](#preprocessing_light)\n",
    "- ### [Preprocessing-Pandas](#preprocessing_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d062bf7",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_light'></a>\n",
    "# Preprocessing-Light"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd824898",
   "metadata": {},
   "source": [
    "---\n",
    "## Decoding\n",
    "converting a sequence of bytes into a sequence of characters.\n",
    "\n",
    "- **Unpacking** \\\n",
    "*.plain/.zip/.gz/...*\n",
    "- **Encoding** \\\n",
    "*ASCII/utf-8/Windows-1251/...*\n",
    "- **Format** \\\n",
    "*csv/xml/json/doc/...*\n",
    "\n",
    "---\n",
    "\n",
    "## Split into tokens\n",
    "splitting a sequence of characters into parts (tokens), possibly excluding some characters from consideration.\n",
    "Naive approach: split the string with spaces and throw out punctuation marks.\n",
    "\n",
    "**Problems:**  \n",
    "* example@example.com, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Language dependency (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "\n",
    "Alternative: n-grams\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92049662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf9412ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      ",\n",
      "and\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sequence = 'The quick brown fox jumps, and jumps over the lazy dog'\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "for token in tokenizer.tokenize(sequence):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e2b4b",
   "metadata": {},
   "source": [
    "---\n",
    "## Stop words\n",
    "the most frequent words in the language that do not contain any information about the content of the text\n",
    "\n",
    "**Problem**: To be or not to be.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6264d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(' '.join(stopwords.words('english')[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804b296",
   "metadata": {},
   "source": [
    "---\n",
    "## Normalization\n",
    "Bringing tokens to a single form in order to get rid of superficial differences in spelling\n",
    "\n",
    "**Approaches:**\n",
    "* formulate a set of rules by which the token is transformed \\\n",
    "New-Yorker → new-yorker → newyorker → newyork\n",
    "* explicity store connections betweens tokens (WordNet - Princeton) \\\n",
    "car → auto, Window 6 → window\n",
    "машина → автомобиль, Windows 6→ window\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d14a2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New-Yorker → new-yorker → newyorker → newyork\n"
     ]
    }
   ],
   "source": [
    "word = 'New-Yorker'\n",
    "\n",
    "word_1 = word.lower()\n",
    "\n",
    "import re\n",
    "word_2 = re.sub(r'\\W', '', word_1, flags = re.U)\n",
    "\n",
    "word_3 = re.sub(r'er', '', word_2, flags = re.U)\n",
    "\n",
    "print(f'{word} → {word_1} → {word_2} → {word_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb6604",
   "metadata": {},
   "source": [
    "---\n",
    "## Stemming and Lemmatization\n",
    "**Stemming** is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\n",
    "**Lemmatization** considers the conext and converts the word to its meaningful base form, which is called *Lemma*.\n",
    "\n",
    "**Example:**\n",
    "* Steeming\n",
    "Caring → Car\n",
    "* Lemmatization\n",
    "Caring → Care\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850f5c2",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a217385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Porter Stemmer]: new-york\n",
      "[Porter Stemmer]: token\n",
      "[English Stemmer]: perfect\n",
      "[English Stemmer]: differ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"New-Yorker\")}')\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"Tokenization\")}')\n",
    "\n",
    "eng_stemmer = EnglishStemmer()\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Perfection\")}')\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Difference\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5dc684",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33696a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pymorphy2]: new-yorker\n",
      "[pymorphy2]: tokenization\n",
      "[pymorphy2]: perfection\n",
      "[pymorphy2]: difference\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(f'[pymorphy2]: {morph.parse(\"New-Yorker\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Tokenization\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Perfection\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Difference\")[0].normal_form}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01471adb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Heap's Law (Herdan's law)\n",
    "An empirical regularity in linguistics that describes the distribution of the number of unique words in a document (or set of documents) as a function of it's length.\n",
    "\n",
    "\n",
    "$ M = kT^{\\beta}$\n",
    "- $M$ - dictionary size\n",
    "- $T$ - word count\n",
    "- $30 \\leq k \\leq 100, b \\approx 0.5$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca473b8",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb208ff0",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_pandas'></a>\n",
    "# Preprocessing-Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0913c2",
   "metadata": {},
   "source": [
    "#### Using methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05aaf7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE]: Our mother washed - The Dishes\n",
      "[AFTER]: ['our', 'mother', 'washed', '-', 'the', 'dishes']\n"
     ]
    }
   ],
   "source": [
    "sequences_list = pd.Series(['Our mother washed - The Dishes', 'The countdown, Is over'], dtype = \"string\")\n",
    "print(f'[BEFORE]: {sequences_list[0]}')\n",
    "\n",
    "sequences_list = sequences_list.str.lower()\n",
    "sequences_list = sequences_list.str.strip()\n",
    "#sequences_list = sequences_list.str.split(' ', expand = True)\n",
    "sequences_list = sequences_list.str.split(' ')\n",
    "print(f'[AFTER]: {sequences_list[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a99fa3",
   "metadata": {},
   "source": [
    "#### Using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ce2ef4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE]: Our mother washed - The Dishes\n",
      "[AFTER]: ['our', 'mother', 'washed', 'the']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pymorphy2\n",
    "\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "sw = ['dishes']\n",
    "\n",
    "def preprocess_txt(line):\n",
    "    exclude = set(string.punctuation)\n",
    "    spls = ''.join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != '']\n",
    "    return spls\n",
    "    \n",
    "sequences_list = pd.Series(['Our mother washed - The Dishes', 'The countdown, Is over'], dtype = \"string\")\n",
    "print(f'[BEFORE]: {sequences_list[0]}')\n",
    "sequences_list = sequences_list.apply(lambda x: preprocess_txt(x))\n",
    "print(f'[AFTER]: {sequences_list[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9278c0",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c71025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370af33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638ca09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e2643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5acc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5a2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914ba50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dccc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921af7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a2c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d0948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051aa59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f7015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "515c4a13",
   "metadata": {},
   "source": [
    "<a id = 'kmeans_inertia'></a>\n",
    "<left>\n",
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border: 0px;\n",
    "           border-bottom: 2px solid #AAA;\n",
    "           font-size:80%;\n",
    "           letter-spacing:0.5px\">\n",
    "<h2 style=\"padding: 10px;\n",
    "           color:#212121;\">Inertia\n",
    "</h2>\n",
    "</div>    \n",
    "</left>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
