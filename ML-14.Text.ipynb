{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc532a9",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "# Table of Contents\n",
    "\n",
    "- ### [Preprocessing-Light](#preprocessing_light)\n",
    "- ### [Preprocessing-Pandas](#preprocessing_pandas)\n",
    "- ### [Word vectorization](#word_vectorization)\n",
    "- ### [Example 1](#example_1)\n",
    "- ### [Gensim](#gensim)\n",
    "- ### [Text Classification](#text_classification)\n",
    "- ### [Example 2: ChatBotPractice](#example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d062bf7",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_light'></a>\n",
    "# Preprocessing-Light"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cecbfcd",
   "metadata": {},
   "source": [
    "---\n",
    "## Decoding\n",
    "converting a sequence of bytes into a sequence of characters.\n",
    "\n",
    "- **Unpacking** \\\n",
    "*.plain/.zip/.gz/...*\n",
    "- **Encoding** \\\n",
    "*ASCII/utf-8/Windows-1251/...*\n",
    "- **Format** \\\n",
    "*csv/xml/json/doc/...*\n",
    "\n",
    "---\n",
    "\n",
    "## Split into tokens\n",
    "splitting a sequence of characters into parts (tokens), possibly excluding some characters from consideration.\n",
    "Naive approach: split the string with spaces and throw out punctuation marks.\n",
    "\n",
    "**Problems:**  \n",
    "* example@example.com, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Language dependency (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "\n",
    "Alternative: n-grams\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cc9794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3ab44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      ",\n",
      "and\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sequence = 'The quick brown fox jumps, and jumps over the lazy dog'\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "for token in tokenizer.tokenize(sequence):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62e105",
   "metadata": {},
   "source": [
    "---\n",
    "## Stop words\n",
    "the most frequent words in the language that do not contain any information about the content of the text\n",
    "\n",
    "**Problem**: To be or not to be.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43b6492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(' '.join(stopwords.words('english')[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9357de5",
   "metadata": {},
   "source": [
    "---\n",
    "## Normalization\n",
    "Bringing tokens to a single form in order to get rid of superficial differences in spelling\n",
    "\n",
    "**Approaches:**\n",
    "* formulate a set of rules by which the token is transformed \\\n",
    "New-Yorker → new-yorker → newyorker → newyork\n",
    "* explicity store connections betweens tokens (WordNet - Princeton) \\\n",
    "car → auto, Window 6 → window\n",
    "машина → автомобиль, Windows 6→ window\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae967b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New-Yorker → new-yorker → newyorker → newyork\n"
     ]
    }
   ],
   "source": [
    "word = 'New-Yorker'\n",
    "\n",
    "word_1 = word.lower()\n",
    "\n",
    "import re\n",
    "word_2 = re.sub(r'\\W', '', word_1, flags = re.U)\n",
    "\n",
    "word_3 = re.sub(r'er', '', word_2, flags = re.U)\n",
    "\n",
    "print(f'{word} → {word_1} → {word_2} → {word_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeae06b",
   "metadata": {},
   "source": [
    "---\n",
    "## Stemming and Lemmatization\n",
    "**Stemming** is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\n",
    "**Lemmatization** considers the conext and converts the word to its meaningful base form, which is called *Lemma*.\n",
    "\n",
    "**Example:**\n",
    "* Steeming\n",
    "Caring → Car\n",
    "* Lemmatization\n",
    "Caring → Care\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e05f22",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfeca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Porter Stemmer]: new-york\n",
      "[Porter Stemmer]: token\n",
      "[English Stemmer]: perfect\n",
      "[English Stemmer]: differ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"New-Yorker\")}')\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"Tokenization\")}')\n",
    "\n",
    "eng_stemmer = EnglishStemmer()\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Perfection\")}')\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Difference\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af3c8c",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0278b1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pymorphy2]: new-yorker\n",
      "[pymorphy2]: tokenization\n",
      "[pymorphy2]: perfection\n",
      "[pymorphy2]: difference\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(f'[pymorphy2]: {morph.parse(\"New-Yorker\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Tokenization\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Perfection\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Difference\")[0].normal_form}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58005d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Heap's Law (Herdan's law)\n",
    "An empirical regularity in linguistics that describes the distribution of the number of unique words in a document (or set of documents) as a function of it's length.\n",
    "\n",
    "\n",
    "$ M = kT^{\\beta}$\n",
    "- $M$ - dictionary size\n",
    "- $T$ - word count\n",
    "- $30 \\leq k \\leq 100, b \\approx 0.5$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bdd03",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c27c6",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_pandas'></a>\n",
    "# Preprocessing-Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8702472",
   "metadata": {},
   "source": [
    "#### Using methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e003fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE]: Our mother washed - The Dishes\n",
      "[AFTER]: ['our', 'mother', 'washed', '-', 'the', 'dishes']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sequences_list = pd.Series(['Our mother washed - The Dishes', 'The countdown, Is over'], dtype = \"string\")\n",
    "print(f'[BEFORE]: {sequences_list[0]}')\n",
    "\n",
    "sequences_list = sequences_list.str.lower()\n",
    "sequences_list = sequences_list.str.strip()\n",
    "#sequences_list = sequences_list.str.split(' ', expand = True)\n",
    "sequences_list = sequences_list.str.split(' ')\n",
    "print(f'[AFTER]: {sequences_list[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cd15d",
   "metadata": {},
   "source": [
    "#### Using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c921e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE]: Our mother washed - The Dishes\n",
      "[AFTER]: ['our', 'mother', 'washed', 'the']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pymorphy2\n",
    "\n",
    "morpher = pymorphy2.MorphAnalyzer()\n",
    "sw = ['dishes']\n",
    "\n",
    "def preprocess_txt(line):\n",
    "    exclude = set(string.punctuation)\n",
    "    spls = ''.join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != '']\n",
    "    return spls\n",
    "    \n",
    "sequences_list = pd.Series(['Our mother washed - The Dishes', 'The countdown, Is over'], dtype = \"string\")\n",
    "print(f'[BEFORE]: {sequences_list[0]}')\n",
    "sequences_list = sequences_list.apply(lambda x: preprocess_txt(x))\n",
    "print(f'[AFTER]: {sequences_list[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12520a",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e0177",
   "metadata": {},
   "source": [
    "<a id = 'word_vectorization'></a>\n",
    "# Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0499a1",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9096b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1     1     0      1     0    0     1    0\n",
       "1        0      0     1   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   1     1     0      0     0    0     1    1\n",
       "3        0      0     0   1     0     1      0     1    1     0    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"I like this movie, it's funny.\", \"I hate this movie.\"\n",
    "             , \"This was awesome! I like it.\", \"Nice one. I love it.\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "count_vectorizer.fit_transform(documents)\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9957f7",
   "metadata": {},
   "source": [
    "### N-gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209a37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'like')\n",
      "('like', 'this')\n",
      "('this', 'movie,')\n",
      "('movie,', \"it's\")\n",
      "(\"it's\", 'funny.')\n",
      "('funny.', 'I')\n",
      "('I', 'hate')\n",
      "('hate', 'this')\n",
      "('this', 'movie.')\n",
      "('movie.', 'This')\n",
      "('This', 'was')\n",
      "('was', 'awesome!')\n",
      "('awesome!', 'I')\n",
      "('I', 'like')\n",
      "('like', 'it.')\n",
      "('it.', 'Nice')\n",
      "('Nice', 'one.')\n",
      "('one.', 'I')\n",
      "('I', 'love')\n",
      "('love', 'it.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n",
    "\n",
    "tokenized = text.split()\n",
    "bigrams = ngrams(tokenized, 2)\n",
    "for i in bigrams:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7200501",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd2eb136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like      love     movie  \\\n",
       "0  0.000000  0.571848  0.000000  0.365003  0.450852  0.000000  0.450852   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
       "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.365003  0.000000  \n",
       "1  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "document = [\"I like this movie, it's funny.\", \"I hate this movie.\"\n",
    "            , \"This was awesome! I like it.\", \"Nice one. I love it.\"]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "feature_name = tfidf_vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e0451",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5588602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1048566</th>\n",
       "      <th>1048567</th>\n",
       "      <th>1048568</th>\n",
       "      <th>1048569</th>\n",
       "      <th>1048570</th>\n",
       "      <th>1048571</th>\n",
       "      <th>1048572</th>\n",
       "      <th>1048573</th>\n",
       "      <th>1048574</th>\n",
       "      <th>1048575</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1048576 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0        1        2        3        4        5        6        7        \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   8        9        ...  1048566  1048567  1048568  1048569  1048570  \\\n",
       "0      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   1048571  1048572  1048573  1048574  1048575  \n",
       "0      0.0      0.0      0.0      0.0      0.0  \n",
       "1      0.0      0.0      0.0      0.0      0.0  \n",
       "2      0.0      0.0      0.0      0.0      0.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[4 rows x 1048576 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "document = [\"I like this movie, it's funny.\", \"I hate this movie.\"\n",
    "            , \"This was awesome! I like it.\", \"Nice one. I love it.\"]\n",
    "\n",
    "vectorizer = HashingVectorizer()\n",
    "values = vectorizer.fit_transform(document)\n",
    "\n",
    "pd.DataFrame(values.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e51ba",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc22b87",
   "metadata": {},
   "source": [
    "<a id = 'example_1'></a>\n",
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3be5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import datasets\n",
    "import tokenizers\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6f098e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98b8ff08c8a4b019edb4a8f664fe027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bee2f8851f4f54ad89a62b0f2e09c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6e8290edce4dbb979dc21dca594673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a000c9ce704fd59ed76543ff218bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0fa471c38f4726ad4fe6b64404c37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8600e7779ad340be839eaa0bf6382acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18f49e4260f44afad4084660a291177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('imdb'\n",
    "                                , split = 'train'\n",
    "                                , download_mode = 'force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0725ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset\n",
    "df = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b221b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4ae18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_labels = [i['label'] for i in df['train']]\n",
    "plt.hist(train_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630a639",
   "metadata": {},
   "source": [
    "### Classification using Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2174c157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>...</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zorak</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zp</th>\n",
       "      <th>zu</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zuniga</th>\n",
       "      <th>zwick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 25000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  007  00s   01   02   03   04   05   06  ...  zooming  zooms  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...      ...    ...   \n",
       "24995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "24996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "24997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "24998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "24999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "\n",
       "       zorak  zorro   zp   zu  zucker  zulu  zuniga  zwick  \n",
       "0        0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "1        0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "2        0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "3        0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "4        0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "...      ...    ...  ...  ...     ...   ...     ...    ...  \n",
       "24995    0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "24996    0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "24997    0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "24998    0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "24999    0.0    0.0  0.0  0.0     0.0   0.0     0.0    0.0  \n",
       "\n",
       "[25000 rows x 25000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 25000)\n",
    "values = vectorizer.fit_transform(df['train']['text'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff36302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<25000x25000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 3355130 stored elements in Compressed Sparse Row format>,\n",
       " array([0, 0, 0, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 25000)\n",
    "vectorizer = vectorizer.fit(df['train']['text'])\n",
    "\n",
    "x_train = vectorizer.transform(df['train']['text'])\n",
    "y_train = np.array(df['train']['label'])\n",
    "\n",
    "x_test = vectorizer.transform(df['test']['text'])\n",
    "y_test = np.array(df['test']['label'])\n",
    "\n",
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762c3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RMSE]: 0.34, [ACC]: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_train, y_train)\n",
    "\n",
    "log_pred = model.predict(x_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, log_pred))\n",
    "acc = accuracy_score(y_test, log_pred)\n",
    "\n",
    "print(f'[RMSE]: {rmse:.2f}, [ACC]: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590063dd",
   "metadata": {},
   "source": [
    "#### Testing model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b56140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PositiveComment]: 1, [NegativeComment]: 0\n"
     ]
    }
   ],
   "source": [
    "positive_comment = 'This movie is awesome!'\n",
    "vec = vectorizer.transform([positive_comment])\n",
    "positive_pred = model.predict(vec)\n",
    "\n",
    "negative_comment = 'This movie is awful!'\n",
    "vec = vectorizer.transform([negative_comment])\n",
    "negative_pred = model.predict(vec)\n",
    "\n",
    "print(f'[PositiveComment]: {positive_pred[0]}, \\\n",
    "[NegativeComment]: {negative_pred[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba2ce4",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc4130",
   "metadata": {},
   "source": [
    "<a id = 'gensim'></a>\n",
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69189a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "result = word_vectors.most_similar(positive = ['woman', 'king'], negative = ['man'])\n",
    "print(word_vectors.doesnt_match('breakfast dinner cereal lunch'.split()))\n",
    "\n",
    "similarity = word_vectors.similarity('woman', 'man')\n",
    "print(f'{similarity:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bbadf",
   "metadata": {},
   "source": [
    "<a id = 'text_classification'></a>\n",
    "# Text Classification\n",
    "\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- LogisticRegression\n",
    "- RandomForestClassifier\n",
    "- XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f1d25980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "#from sklearn import naive_bayes, linear_model, ensemble, svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "%pylab inline\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "229ebce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('Upload/14.7.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9197d253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>It just works!</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>В целом удобноное приложение...из минусов хотя...</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Отлично все</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Стал зависать на 1% работы антивируса. Дальше ...</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Очень удобно, работает быстро.</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                            Content        Date\n",
       "0       5                                     It just works!  2017-08-14\n",
       "1       4  В целом удобноное приложение...из минусов хотя...  2017-08-14\n",
       "2       5                                        Отлично все  2017-08-14\n",
       "3       5  Стал зависать на 1% работы антивируса. Дальше ...  2017-08-14\n",
       "4       5                     Очень удобно, работает быстро.  2017-08-14"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "29f8623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20659 entries, 0 to 20658\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Rating   20659 non-null  int64 \n",
      " 1   Content  20656 non-null  object\n",
      " 2   Date     20659 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 484.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f23f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(punctuation)\n",
    "sw = set(get_stop_words('ru'))\n",
    "morpher = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "13d79f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.iloc[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f9c938ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(line):\n",
    "    splt = str(line)\n",
    "    splt = ''.join(i for i in splt if i not in exclude)\n",
    "    splt = splt.lower()\n",
    "    splt = re.sub('не\\s', 'не', splt)\n",
    "    splt = [morpher.parse(word)[0].normal_form for word in splt.split() if word not in exclude]\n",
    "    return ' '.join(splt)\n",
    "\n",
    "#df_sample['Content'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "70cbf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_12192\\725875120.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample['text'] = df_sample['Content'].apply(preprocess_text)\n",
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_12192\\725875120.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample_rating['target'] = df_sample_rating['Rating'] > 3\n"
     ]
    }
   ],
   "source": [
    "df_sample['text'] = df_sample['Content'].apply(preprocess_text)\n",
    "df_sample_rating = df_sample[df_sample['Rating'] != 3]\n",
    "df_sample_rating['target'] = df_sample_rating['Rating'] > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f18356e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, train_y, test_y = train_test_split(df_sample_rating['text']\n",
    "                                                                    , df_sample_rating['target'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(train_y)\n",
    "y_test = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6e75c65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3585,), (1195,))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "614ed3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer().fit(x_train.values)\n",
    "x_train_count = count_vectorizer.transform(x_train)\n",
    "x_test_count = count_vectorizer.transform(x_test)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer().fit(x_train.values)\n",
    "x_train_tfidf = tfidf_vectorizer.transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "tfidf_vectorizer_ngram = TfidfVectorizer(ngram_range = (1, 3)).fit(x_train.values)\n",
    "x_train_tfidf_ngram = tfidf_vectorizer_ngram.transform(x_train)\n",
    "x_test_tfidf_ngram = tfidf_vectorizer_ngram.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8423ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_test, is_neural_net = False):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    predictions = classifier.predict(feature_vector_test)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis = -1)\n",
    "        \n",
    "    return accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb315a",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3f59a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Count]: 0.93, [TF-IDF]: 0.89, [TF-IDF N-Gram]: 0.87\n"
     ]
    }
   ],
   "source": [
    "acc_nb_count = train_model(naive_bayes.MultinomialNB(), x_train_count, y_train, x_test_count)\n",
    "acc_nb_tfidf = train_model(naive_bayes.MultinomialNB(), x_train_tfidf, y_train, x_test_tfidf)\n",
    "acc_nb_tfidf_ngram = train_model(naive_bayes.MultinomialNB(), x_train_tfidf_ngram, y_train, x_test_tfidf_ngram)\n",
    "\n",
    "print(f'[Count]: {acc_nb_count:.2f}, [TF-IDF]: {acc_nb_tfidf:.2f}, [TF-IDF N-Gram]: {acc_nb_tfidf_ngram:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf37ba",
   "metadata": {},
   "source": [
    "## Linear Classifier - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a98b37d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Count]: 0.91, [TF-IDF]: 0.90, [TF-IDF N-Gram]: 0.90\n"
     ]
    }
   ],
   "source": [
    "acclrcount = train_model(LogisticRegression(), x_train_count, y_train, x_test_count)\n",
    "acclrtfidf = train_model(LogisticRegression(), x_train_tfidf, y_train, x_test_tfidf)\n",
    "acclrtfidf_ngram = train_model(LogisticRegression(), x_train_tfidf_ngram, y_train, x_test_tfidf_ngram)\n",
    "\n",
    "print(f'[Count]: {acclrcount:.2f}, [TF-IDF]: {acclrtfidf:.2f}, [TF-IDF N-Gram]: {acclrtfidf_ngram:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eaeb40",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8ec831ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Count]: 0.91, [TF-IDF]: 0.91, [TF-IDF N-Gram]: 0.90\n"
     ]
    }
   ],
   "source": [
    "acc_svm_count = train_model(svm.SVC(), x_train_count, y_train, x_test_count)\n",
    "acc_svm_tfidf = train_model(svm.SVC(), x_train_tfidf, y_train, x_test_tfidf)\n",
    "acc_svm_tfidf_ngram = train_model(svm.SVC(), x_train_tfidf_ngram, y_train, x_test_tfidf_ngram)\n",
    "\n",
    "print(f'[Count]: {acc_svm_count:.2f}, [TF-IDF]: {acc_svm_tfidf:.2f}, [TF-IDF N-Gram]: {acc_svm_tfidf_ngram:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49dc6a",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "94778de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ensemble.RandomForestClassifier(n_estimators = 10000).fit(x_train_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3b095ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_12192\\4151042975.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook([10, 30, 60, 100, 1000, 2000]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b800066e689f45c5a59c9f73d21c74cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_scores = []\n",
    "\n",
    "for i in tqdm_notebook([10, 30, 60, 100, 1000, 2000]):\n",
    "    clf =  ensemble.RandomForestClassifier(n_estimators = i).fit(x_train_count, y_train)\n",
    "    acc_scores.append(accuracy_score(clf.predict(x_test_count), y_test))\n",
    "    \n",
    "acc_rfc_count = np.mean(acc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e2dcf4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Count]: 0.90, [TF-IDF]: 0.90, [TF-IDF N-Gram]: 0.90\n"
     ]
    }
   ],
   "source": [
    "acc_rfc_tfidf = train_model(ensemble.RandomForestClassifier(), x_train_tfidf, y_train, x_test_tfidf)\n",
    "acc_rfc_tfidf_ngram = train_model(ensemble.RandomForestClassifier(), x_train_tfidf_ngram, y_train, x_test_tfidf_ngram)\n",
    "\n",
    "print(f'[Count]: {acc_rfc_count:.2f}, [TF-IDF]: {acc_rfc_tfidf:.2f}, [TF-IDF N-Gram]: {acc_rfc_tfidf_ngram:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fda59d",
   "metadata": {},
   "source": [
    "## Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6e85ef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Count]: 0.91, [TF-IDF]: 0.90, [TF-IDF N-Gram]: 0.90\n"
     ]
    }
   ],
   "source": [
    "acc_xgb_count = train_model(XGBClassifier(), x_train_count, y_train, x_test_count)\n",
    "acc_xgb_tfidf = train_model(XGBClassifier(), x_train_tfidf, y_train, x_test_tfidf)\n",
    "acc_xgb_tfidf_ngram = train_model(XGBClassifier(), x_train_tfidf_ngram, y_train, x_test_tfidf_ngram)\n",
    "\n",
    "print(f'[Count]: {acc_xgb_count:.2f}, [TF-IDF]: {acc_xgb_tfidf:.2f}, [TF-IDF N-Gram]: {acc_xgb_tfidf_ngram:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6386d",
   "metadata": {},
   "source": [
    "<a id = 'example_2'></a>\n",
    "# Example 2: ChatBotPractice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5471ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os \n",
    "import annoy\n",
    "import codecs\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import Word2Vec\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2fb149c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616bab9f61284a1fb499a144cf449b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 0\n",
    "end = 1000\n",
    "\n",
    "question = None\n",
    "written = False\n",
    "\n",
    "with codecs.open('Upload/prepared_answers.txt', 'w', 'utf-8') as f_out:\n",
    "    with codecs.open('Upload/14.8.txt', 'r', 'utf-8') as f_in:\n",
    "        for line in tqdm(f_in):\n",
    "            start += 1\n",
    "            if start > end:\n",
    "                break\n",
    "                \n",
    "            if line.startswith('---'):\n",
    "                written = False\n",
    "            if not written and question is not None:\n",
    "                f_out.write(question.replace('\\t', ' ').strip() + '\\t' + line.replace('\\t',' '))\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = None\n",
    "written = False\n",
    "\n",
    "#Мы идем по всем записям, берем первую строку как вопрос\n",
    "# и после знака --- находим ответ\n",
    "with codecs.open(\"prepared_answers.txt\",\"w\", \"utf-8\") as fout:\n",
    "    with codecs.open(\"Otvety.txt\", \"r\", \"utf-8\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            if line.startswith(\"---\"):\n",
    "                written = False\n",
    "                continue\n",
    "            if not written and question is not None:\n",
    "                fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                written = True\n",
    "                question = None\n",
    "                continue\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0b9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cab3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914a58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec5441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1067c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3524d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c50fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edca6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916ac44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c195c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcb89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c8ba4cf",
   "metadata": {},
   "source": [
    "<a id = 'kmeans_inertia'></a>\n",
    "<left>\n",
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border: 0px;\n",
    "           border-bottom: 2px solid #AAA;\n",
    "           font-size:80%;\n",
    "           letter-spacing:0.5px\">\n",
    "<h2 style=\"padding: 10px;\n",
    "           color:#212121;\">Inertia\n",
    "</h2>\n",
    "</div>    \n",
    "</left>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
