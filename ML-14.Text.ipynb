{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc532a9",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "# Table of Contents\n",
    "\n",
    "- ### [Preprocessing-Light](#preprocessing_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d062bf7",
   "metadata": {},
   "source": [
    "<a id = 'preprocessing_light'></a>\n",
    "# Preprocessing-Light"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412bd1d",
   "metadata": {},
   "source": [
    "---\n",
    "## Decoding\n",
    "converting a sequence of bytes into a sequence of characters.\n",
    "\n",
    "- **Unpacking** \\\n",
    "*.plain/.zip/.gz/...*\n",
    "- **Encoding** \\\n",
    "*ASCII/utf-8/Windows-1251/...*\n",
    "- **Format** \\\n",
    "*csv/xml/json/doc/...*\n",
    "\n",
    "---\n",
    "\n",
    "## Split into tokens\n",
    "splitting a sequence of characters into parts (tokens), possibly excluding some characters from consideration.\n",
    "Naive approach: split the string with spaces and throw out punctuation marks.\n",
    "\n",
    "**Problems:**  \n",
    "* example@example.com, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Language dependency (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "\n",
    "Alternative: n-grams\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f2f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9563ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumps\n",
      ",\n",
      "and\n",
      "jumps\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sequence = 'The quick brown fox jumps, and jumps over the lazy dog'\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "for token in tokenizer.tokenize(sequence):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749c1f9",
   "metadata": {},
   "source": [
    "---\n",
    "## Stop words\n",
    "the most frequent words in the language that do not contain any information about the content of the text\n",
    "\n",
    "**Problem**: To be or not to be.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09facad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(' '.join(stopwords.words('english')[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc4c749",
   "metadata": {},
   "source": [
    "---\n",
    "## Normalization\n",
    "Bringing tokens to a single form in order to get rid of superficial differences in spelling\n",
    "\n",
    "**Approaches:**\n",
    "* formulate a set of rules by which the token is transformed \\\n",
    "New-Yorker → new-yorker → newyorker → newyork\n",
    "* explicity store connections betweens tokens (WordNet - Princeton) \\\n",
    "car → auto, Window 6 → window\n",
    "машина → автомобиль, Windows 6→ window\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5119f8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New-Yorker → new-yorker → newyorker → newyork\n"
     ]
    }
   ],
   "source": [
    "word = 'New-Yorker'\n",
    "\n",
    "word_1 = word.lower()\n",
    "\n",
    "import re\n",
    "word_2 = re.sub(r'\\W', '', word_1, flags = re.U)\n",
    "\n",
    "word_3 = re.sub(r'er', '', word_2, flags = re.U)\n",
    "\n",
    "print(f'{word} → {word_1} → {word_2} → {word_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62442a",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee53440",
   "metadata": {},
   "source": [
    "---\n",
    "## Stemming and Lemmatization\n",
    "**Stemming** is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\n",
    "**Lemmatization** considers the conext and converts the word to its meaningful base form, which is called *Lemma*.\n",
    "\n",
    "**Example:**\n",
    "* Steeming\n",
    "Caring → Car\n",
    "* Lemmatization\n",
    "Caring → Care\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a39560",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccf52f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Porter Stemmer]: new-york\n",
      "[Porter Stemmer]: token\n",
      "[English Stemmer]: perfect\n",
      "[English Stemmer]: differ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"New-Yorker\")}')\n",
    "print(f'[Porter Stemmer]: {p_stemmer.stem(\"Tokenization\")}')\n",
    "\n",
    "eng_stemmer = EnglishStemmer()\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Perfection\")}')\n",
    "print(f'[English Stemmer]: {eng_stemmer.stem(\"Difference\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a49fb",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a8c88b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pymorphy2]: new-yorker\n",
      "[pymorphy2]: tokenization\n",
      "[pymorphy2]: perfection\n",
      "[pymorphy2]: difference\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(f'[pymorphy2]: {morph.parse(\"New-Yorker\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Tokenization\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Perfection\")[0].normal_form}')\n",
    "print(f'[pymorphy2]: {morph.parse(\"Difference\")[0].normal_form}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802929f",
   "metadata": {},
   "source": [
    "[UP](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb140d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Heap's Law (Herdan's law)\n",
    "An empirical regularity in linguistics that describes the distribution of the number of unique words in a document (or set of documents) as a function of it's length.\n",
    "\n",
    "\n",
    "$ M = kT^{\\beta}$\n",
    "- $M$ - dictionary size\n",
    "- $T$ - word count\n",
    "- $30 \\leq k \\leq 100, b \\approx 0.5$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462e732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee4bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a60678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93bcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101eec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330bd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a38b9f01",
   "metadata": {},
   "source": [
    "<a id = 'kmeans_inertia'></a>\n",
    "<left>\n",
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border: 0px;\n",
    "           border-bottom: 2px solid #AAA;\n",
    "           font-size:80%;\n",
    "           letter-spacing:0.5px\">\n",
    "<h2 style=\"padding: 10px;\n",
    "           color:#212121;\">Inertia\n",
    "</h2>\n",
    "</div>    \n",
    "</left>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
